{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOku8APeumHVOu+NmYuFER3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarparab28/Dy.Tech/blob/main/Default_Transformer_based_recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "dZDgvkqiGaVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "dz9W3NWfBrno"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import math\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.layers import StringLookup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Prepration**"
      ],
      "metadata": {
        "id": "_7E1Lf-TGkcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
        "ZipFile(\"movielens.zip\", \"r\").extractall()"
      ],
      "metadata": {
        "id": "Nd3wWKhNCbXz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(\n",
        "    \"ml-1m/users.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")\n",
        "\n",
        "ratings = pd.read_csv(\n",
        "    \"ml-1m/ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"ml-1m/movies.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"movie_id\", \"title\", \"genres\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")"
      ],
      "metadata": {
        "id": "IpC3hI4SCguP"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
        "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
        "\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))"
      ],
      "metadata": {
        "id": "NeX5kwPlCzp9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\"]\n",
        "genres += [\"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\"]\n",
        "genres += [\"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
        "\n",
        "for genre in genres:\n",
        "    movies[genre] = movies[\"genres\"].apply(\n",
        "        lambda values: int(genre in values.split(\"|\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "-qjxGMKaC2T5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transform the movie ratings data into sequences**"
      ],
      "metadata": {
        "id": "xxziab-NG0x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
        "\n",
        "ratings_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),\n",
        "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
        "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
        "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "hwQhZFscC6Pu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "step_size = 2\n",
        "\n",
        "\n",
        "def create_sequences(values, window_size, step_size):\n",
        "    sequences = []\n",
        "    start_index = 0\n",
        "    while True:\n",
        "        end_index = start_index + window_size\n",
        "        seq = values[start_index:end_index]\n",
        "        if len(seq) < window_size:\n",
        "            seq = values[-window_size:]\n",
        "            if len(seq) == window_size:\n",
        "                sequences.append(seq)\n",
        "            break\n",
        "        sequences.append(seq)\n",
        "        start_index += step_size\n",
        "    return sequences\n",
        "\n",
        "\n",
        "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "ratings_data.ratings = ratings_data.ratings.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "del ratings_data[\"timestamps\"]"
      ],
      "metadata": {
        "id": "DImbdvJ0C-Fg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
        "    \"movie_ids\", ignore_index=True\n",
        ")\n",
        "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
        "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
        "ratings_data_transformed = ratings_data_transformed.join(\n",
        "    users.set_index(\"user_id\"), on=\"user_id\"\n",
        ")\n",
        "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
        "    lambda x: \",\".join(x)\n",
        ")\n",
        "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
        "    lambda x: \",\".join([str(v) for v in x])\n",
        ")\n",
        "\n",
        "del ratings_data_transformed[\"zip_code\"]\n",
        "\n",
        "ratings_data_transformed.rename(\n",
        "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xS_pYUI7DBya"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
        "train_data = ratings_data_transformed[random_selection]\n",
        "test_data = ratings_data_transformed[~random_selection]\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)"
      ],
      "metadata": {
        "id": "fNy-vLjPDEZo"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define metadata**"
      ],
      "metadata": {
        "id": "ErmIipiCG8gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER = list(ratings_data_transformed.columns)\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"user_id\": list(users.user_id.unique()),\n",
        "    \"movie_id\": list(movies.movie_id.unique()),\n",
        "    \"sex\": list(users.sex.unique()),\n",
        "    \"age_group\": list(users.age_group.unique()),\n",
        "    \"occupation\": list(users.occupation.unique()),\n",
        "}\n",
        "\n",
        "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
        "\n",
        "MOVIE_FEATURES = [\"genres\"]"
      ],
      "metadata": {
        "id": "Pll9eoINDHiy"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create tf.data.Dataset for training and evaluation**"
      ],
      "metadata": {
        "id": "oA-bh7wOHDIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    def process(features):\n",
        "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
        "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
        "\n",
        "        # The last movie id in the sequence is the target movie.\n",
        "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
        "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
        "\n",
        "        ratings_string = features[\"sequence_ratings\"]\n",
        "        sequence_ratings = tf.strings.to_number(\n",
        "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
        "        ).to_tensor()\n",
        "\n",
        "        # The last rating in the sequence is the target for the model to predict.\n",
        "        target = sequence_ratings[:, -1]\n",
        "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
        "\n",
        "        return features, target\n",
        "\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        field_delim=\"|\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(process)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "xcViMNYeDJQb"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create model inputs**"
      ],
      "metadata": {
        "id": "f-ZKa7UoHI7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
        "        \"sequence_movie_ids\": keras.Input(\n",
        "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"target_movie_id\": keras.Input(\n",
        "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"sequence_ratings\": keras.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
        "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
        "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
        "    }"
      ],
      "metadata": {
        "id": "tk4D5A38DPpf"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encode input features**"
      ],
      "metadata": {
        "id": "Uv1GtZ8SHLkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        movie_idx = movie_index_lookup(movie_id)\n",
        "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
        "        encoded_movie = movie_embedding\n",
        "        if include_movie_features:\n",
        "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
        "            encoded_movie = movie_embedding_processor(\n",
        "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
        "            )\n",
        "        return encoded_movie\n",
        "         ## Encoding target_movie_id\n",
        "    target_movie_id = inputs[\"target_movie_id\"]\n",
        "    encoded_target_movie = encode_movie(target_movie_id)\n",
        "\n",
        "    ## Encoding sequence movie_ids.\n",
        "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
        "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
        "    # Create positional embedding.\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encodded_positions = position_embedding_encoder(positions)\n",
        "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
        "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
        "    sequence_ratings = keras.ops.expand_dims(sequence_ratings, -1)\n",
        "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
        "    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs.\n",
        "    for i in range(sequence_length - 1):\n",
        "        feature = encoded_sequence_movies_with_poistion_and_rating[:, i, ...]\n",
        "        feature = keras.ops.expand_dims(feature, 1)\n",
        "        encoded_transformer_features.append(feature)\n",
        "    encoded_transformer_features.append(encoded_target_movie)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features"
      ],
      "metadata": {
        "id": "FuBZasTBHQgf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create a BST model**"
      ],
      "metadata": {
        "id": "iRz2KOIQHmEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "\n",
        "# Define global variables\n",
        "include_user_id = False\n",
        "include_user_features = False\n",
        "include_movie_features = False\n",
        "\n",
        "hidden_units = [256, 128]\n",
        "dropout_rate = 0.1\n",
        "num_heads = 3\n",
        "\n",
        "# Function to create model inputs\n",
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
        "        \"sequence_movie_ids\": keras.Input(\n",
        "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"target_movie_id\": keras.Input(\n",
        "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"sequence_ratings\": keras.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
        "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
        "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
        "    }\n",
        "\n",
        "# Function to encode input features\n",
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        movie_idx = movie_index_lookup(movie_id)\n",
        "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
        "        encoded_movie = movie_embedding\n",
        "        if include_movie_features:\n",
        "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
        "            encoded_movie = movie_embedding_processor(\n",
        "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
        "            )\n",
        "        return encoded_movie\n",
        "\n",
        "    ## Encoding target_movie_id\n",
        "    target_movie_id = inputs[\"target_movie_id\"]\n",
        "    encoded_target_movie = encode_movie(target_movie_id)\n",
        "\n",
        "    ## Encoding sequence_movie_ids\n",
        "    sequence_movie_ids = inputs[\"sequence_movie_ids\"]\n",
        "    encoded_sequence_movies = encode_movie(sequence_movie_ids)\n",
        "\n",
        "    ## Create positional embedding\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encoded_positions = position_embedding_encoder(positions)\n",
        "\n",
        "    ## Retrieve sequence ratings to incorporate them into the movie encoding\n",
        "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
        "    sequence_ratings = tf.expand_dims(sequence_ratings, -1)\n",
        "\n",
        "    ## Add positional encoding to movie encodings and multiply them by rating\n",
        "    encoded_sequence_movies_with_position_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_movies + encoded_positions), sequence_ratings]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs\n",
        "    for i in range(sequence_length - 1):\n",
        "        feature = encoded_sequence_movies_with_position_and_rating[:, i, ...]\n",
        "        feature = tf.expand_dims(feature, 1)\n",
        "        encoded_transformer_features.append(feature)\n",
        "    encoded_transformer_features.append(encoded_target_movie)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features\n",
        "\n",
        "# Function to create the model\n",
        "def create_model():\n",
        "    inputs = create_model_inputs()\n",
        "    transformer_features, other_features = encode_input_features(\n",
        "        inputs, include_user_id, include_user_features, include_movie_features\n",
        "    )\n",
        "\n",
        "    # Create a multi-headed attention layer\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
        "    )(transformer_features, transformer_features)\n",
        "\n",
        "    # Transformer block\n",
        "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
        "    x1 = layers.Add()([transformer_features, attention_output])\n",
        "    x1 = layers.LayerNormalization()(x1)\n",
        "    x2 = layers.LeakyReLU()(x1)\n",
        "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
        "    x2 = layers.Dropout(dropout_rate)(x2)\n",
        "    transformer_features = layers.Add()([x1, x2])\n",
        "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
        "    features = layers.Flatten()(transformer_features)\n",
        "\n",
        "    # Include the other features\n",
        "    if other_features is not None:\n",
        "        features = layers.concatenate(\n",
        "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
        "        )\n",
        "\n",
        "    # Fully-connected layers\n",
        "    for num_units in hidden_units:\n",
        "        features = layers.Dense(num_units)(features)\n",
        "        features = layers.BatchNormalization()(features)\n",
        "        features = layers.LeakyReLU()(features)\n",
        "        features = layers.Dropout(dropout_rate)(features)\n",
        "\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model()"
      ],
      "metadata": {
        "id": "DgRAcUeKHaPA"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),  # Use Adagrad optimizer with learning rate 0.01\n",
        "    loss=keras.losses.MeanSquaredError(),  # Mean Squared Error loss function\n",
        "    metrics=[keras.metrics.MeanAbsoluteError()],  # Mean Absolute Error as the evaluation metric\n",
        ")\n",
        "\n",
        "# Read the training data.\n",
        "train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n",
        "\n",
        "# Fit the model with the training data.\n",
        "model.fit(train_dataset, epochs=5)  # Train the model for 5 epochs\n",
        "\n",
        "# Read the test data.\n",
        "test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n",
        "\n",
        "# Evaluate the model on the test data.\n",
        "_, mae = model.evaluate(test_dataset, verbose=0)  # Evaluate and get Mean Absolute Error\n",
        "print(f\"Test MAE: {round(mae, 3)}\")  # Print the Mean Absolute Error rounded to 3 decimal places"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR7y-ErdQbY_",
        "outputId": "be3d57de-8976-4aff-eaf0-ca427522b7e1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1598/1598 [==============================] - 109s 64ms/step - loss: 1.2223 - mean_absolute_error: 0.8800\n",
            "Epoch 2/5\n",
            "1598/1598 [==============================] - 102s 63ms/step - loss: 1.0033 - mean_absolute_error: 0.7986\n",
            "Epoch 3/5\n",
            "1598/1598 [==============================] - 101s 63ms/step - loss: 0.9493 - mean_absolute_error: 0.7760\n",
            "Epoch 4/5\n",
            "1598/1598 [==============================] - 86s 54ms/step - loss: 0.9192 - mean_absolute_error: 0.7635\n",
            "Epoch 5/5\n",
            "1598/1598 [==============================] - 84s 53ms/step - loss: 0.9026 - mean_absolute_error: 0.7564\n",
            "Test MAE: 0.776\n"
          ]
        }
      ]
    }
  ]
}